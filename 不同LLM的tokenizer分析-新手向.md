# 不同LLM的tokenizer分析-新手向

### A. LLM中分词的必要性

一个理想的分词策略旨在为模型提供“最有意义”且“尽可能小”的文本表示 。这意味着分词结果既要能捕捉文本的语义信息，又要控制词元序列的长度和词汇表的大小，以实现计算效率和良好的泛化能力。最初，分词方法较为简单，如基于空格和标点符号的词语切分。然而，随着模型复杂度的提升和对更广泛语言现象处理的需求，分词技术也从基于词（word-based）和基于字符（character-based）的方法，演进到更为精细和高效的子词（subword）分词算法。本报告将深入探讨这些主流分词技术的核心原理、实现方式及其在现代LLM中的应用。

### B. 目的与结构概览

1. **分词基础**：介绍传统分词方法的局限性，引出子词分词的必要性，并概述编码与解码过程。
2. **主流子词分词算法详解**：深入剖析字节对编码（Byte-Pair Encoding, BPE）、WordPiece和Unigram三种核心子词算法。
3. **SentencePiece现代分词框架**：阐述SentencePiece的设计理念、关键特性及其与BPE/Unigram的集成。
4. **各分词器对比分析与选择指南**：比较不同分词器的性能特点、适用场景，并讨论如何处理特殊情况，最后为初学者提供选择建议。

## II. 分词基础：从词到子词

在深入探讨复杂的子词分词算法之前，有必要回顾传统的分词方法及其固有的局限性，这将有助于理解为何子词分词会成为现代LLM的主流选择。

### A. 传统分词方法及其局限性

#### 1. 基于词的分词 (Word-based Tokenization)

基于词的分词是最直观的方法，它通常依据空格和标点符号将文本分割成独立的词语 。例如，句子“Jim Henson was a puppeteer”会被切分为词元列表：`['Jim', 'Henson', 'was', 'a', 'puppeteer']` 1。

然而，这种方法面临诸多挑战：

- **巨大的词汇表**：每种语言都包含大量词汇。例如，仅英语就可能有超过50万个独一无二的词。为每个词分配一个唯一的ID会导致词汇表异常庞大，这不仅增加了模型的存储需求，也加大了模型的学习难度 1。
- **语义关联的缺失**：模型最初无法理解词形相近或意义相关的词之间的联系。例如，“dog”与“dogs”，“run”与“running”会被视为完全不同的词元，尽管它们在语义上高度相关 。
- **未登录词 (Out-of-Vocabulary, OOV) 问题**：任何未在训练词汇表中出现的词，在分词时都会被映射为一个特殊的“未知”词元（通常表示为`[UNK]`）。如果文本中`[UNK]`词元过多，模型将丢失大量信息，导致其对文本的理解大打折扣 。这种信息损失对于模型的性能是极为不利的，尤其是在处理生僻词、专有名词或新兴网络用语时。

#### 2. 基于字符的分词 (Character-based Tokenization)

为了克服基于词的分词的某些问题，另一种方法是基于字符的分词，即将文本切分为单个字符 。

这种方法的优势在于：

- **极小的词汇表**：词汇表仅包含所有基本字符（如字母、数字、标点），数量大大减少 。
- **极少的未登录词**：几乎所有词语都可以由字符构成，因此`[UNK]`词元的出现频率显著降低 。

但其缺点同样明显：

- **语义单元的破坏**：单个字符（尤其在字母文字如英语中）通常不携带独立的语义信息。将词语拆分为字符序列，使得模型难以学习到词级别的语义表示。尽管在某些字符本身即为重要语义单元的语言（如中文）中，这种情况有所不同 。
- **词元序列过长**：一个词会被分解为多个字符，导致输入模型的词元序列变得非常长。这不仅增加了计算负担，也可能使模型难以捕捉长距离依赖关系 1。词汇表大小与序列长度之间存在一种权衡：较小的词汇表（如字符级）通常意味着较长的词元序列，反之亦然。这种权衡直接影响模型的训练效率和学习能力。

### B. 子词分词的崛起

子词分词（Subword Tokenization）的出现，旨在结合基于词和基于字符的分词的优点，同时规避它们的缺点 。其核心思想是：常用词应作为独立的词元保留，而稀有词则应被拆分成更小的、有意义的子词单元 。

例如，稀有词“annoyingly”可能被拆分为“annoying”和“ly”；“tokenization”可能被拆分为“token”和“ization” 。这种做法带来了多重益处：

- **保留语义信息**：子词（如“token”，“ization”）往往自身携带一定的语义含义 。
- **空间效率与覆盖率的平衡**：通过子词组合，可以用相对较小的词汇表表示大量词汇，包括未曾见过的词，从而有效控制词汇表规模，同时保持对语料的良好覆盖 。
- **显著减少未登录词**：新词或罕见词可以通过已知的子词组合来表示，大大减少了`[UNK]`词元的出现 。
- **适用于黏着语和屈折变化丰富的语言**：对于那些通过组合多个语素来构成复杂词语的语言（如土耳其语、德语），子词分词尤其有效。

子词分词的成功在于它能够在词汇表大小、OOV处理能力以及语义信息的保留之间取得巧妙的平衡。它不是简单地在词和字符之间做选择，而是创造了一种中间粒度的表示，更适应自然语言的复杂性和多样性。常见的子词分词算法包括字节对编码（BPE）、WordPiece和Unigram等，它们将在后续章节详细介绍。

### C. 编码与解码过程

分词的完整流程不仅仅是文本切分，还包括将词元映射为数字ID（编码）以及将数字ID还原为文本（解码）。

- **编码（Encoding**）：这是将文本转换为模型可处理的数字序列的过程。它通常包含两个步骤 
  1. **分词（Tokenization）**：根据特定分词器的规则（这些规则由预训练的分词模型定义），将输入文本切分为词元列表。例如，Hugging Face Transformers库中的`tokenize()`方法执行此操作，输出一个字符串列表。
  2. **转换为输入ID（Conversion to Input IDs）**：使用分词器的词汇表，将每个词元映射为其对应的唯一数字ID。`convert_tokens_to_ids()`方法负责此步骤，生成一个整数列表。
- **解码（Decoding）**：这是编码的逆过程，即将数字ID序列转换回人类可读的文本字符串。`decode()`方法不仅将ID转换回词元，还能智能地将子词组合成完整的词语和句子 。这对于文本生成、机器翻译、摘要等任务至关重要，因为模型输出的是数字ID序列，需要解码后才能呈现给用户。

分词本质上是一个可逆的映射过程，其核心是维护一个词汇表（vocabulary），这个词汇表定义了从文本单元到数字ID的对应关系。这个过程确保了模型能够以结构化的方式处理和生成语言。

## III. 主流子词分词算法详解

子词分词通过将词汇切分成更小的单元，有效地平衡了词汇表大小和对未登录词的处理能力。本章节将详细介绍三种主流的子词分词算法：BPE、WordPiece和Unigram。

### A. BPE (Byte-Pair Encoding)

#### 核心原理与起源

字节对编码（Byte-Pair Encoding, BPE）最初是一种数据压缩算法，后来被引入自然语言处理领域用于文本分词 。它被广泛应用于许多著名的Transformer模型中，如GPT、GPT-2、RoBERTa、BART和DeBERTa 。BPE的核心思想是从一个基础的字符词汇表开始，通过迭代地合并语料库中出现频率最高的相邻词元对来构建新的、更长的子词词元，直至达到预设的词汇表大小 。

#### 训练步骤详解与示例

BPE的训练过程如下：

1. **预处理与初始词汇表构建**：
   - 首先对输入语料进行规范化（如转小写、处理标点）和预分词（通常是按空格和标点切分成词）。
   - 然后，从预分词后的词集合中提取所有唯一的字符，构成初始的基础词汇表。例如，如果语料包含词 "hug", "pug", "pun", "bun", "hugs"，则基础词汇表为 `["b", "g", "h", "n", "p", "s", "u"]` 。在实际应用中，这个基础词汇表通常包含所有ASCII字符，甚至扩展到部分Unicode字符。
2. **字节级BPE (Byte-level BPE)**：
   - 一个重要变种是字节级BPE，它不直接处理Unicode字符，而是将词语视为字节序列。这意味着基础词汇表固定为256个字节值。这种做法的巨大优势在于它从根本上消除了字符级别的未登录词问题，因为任何文本都可以表示为字节序列。GPT-2和RoBERTa等模型采用了这种策略，确保了对任意输入文本的稳健处理，即使文本中包含训练时未见过的字符或符号。
3. **学习合并规则 (Learning Merges)**：
   - 在构建好初始词汇表后，算法开始学习“合并规则”。一条合并规则定义了如何将词汇表中两个相邻的现有词元组合成一个新的词元。
   - 在训练的每一步，BPE算法会统计当前语料中所有相邻词元对的出现频率，并找出频率最高的那一对。
   - 将这个最高频对合并成一个新的词元，加入词汇表，并将语料中所有该词元对的实例替换为这个新合并的词元。
   - 重复此过程，直到词汇表大小达到预设的目标值（例如30,000或50,000）。

训练示例 ：

假设我们有以下带词频的语料：("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)。

初始词汇表：["b", "g", "h", "n", "p", "s", "u"] (以及其他基础字符)。

初始词语拆分（以空格表示词元间隔）：

("h u g", 10), ("p u g", 5), ("p u n", 12), ("b u n", 4), ("h u g s", 5)

- 第一次合并：
  - 最高频对是 `("u", "g")`，出现 10+5+5=20 次。
  - 学习规则：`("u", "g") -> "ug"`。
  - 词汇表增加 "ug"。
  - 语料更新为：`("h ug", 10), ("p ug", 5), ("p u n", 12), ("b u n", 4), ("h ug s", 5)`。
- 第二次合并：
  - 此时最高频对是 `("u", "n")`，出现 12+4=16 次。
  - 学习规则：`("u", "n") -> "un"`。
  - 词汇表增加 "un"。
  - 语料更新为：`("h ug", 10), ("p ug", 5), ("p un", 12), ("b un", 4), ("h ug s", 5)`。
- 第三次合并：
  - 此时最高频对是 `("h", "ug")`，出现 10+5=15 次。
  - 学习规则：`("h", "ug") -> "hug"`。
  - 词汇表增加 "hug"。
  - 语料更新为：`("hug", 10), ("p ug", 5), ("p un", 12), ("b un", 4), ("hug s", 5)`。 这个过程会持续进行，直到达到目标词汇表大小。值得注意的是，BPE的合并过程是贪婪的，并且基于频率。这意味着它简单直接，但也可能导致合并出一些在语言学上不太直观的子词单元。合并规则的顺序至关重要，因为较早的合并会影响后续合并的候选对及其频率。

#### 分词流程剖析与示例

对新文本进行BPE分词时，过程与训练类似，但使用的是训练阶段学习到的合并规则集合：

1. **规范化与预分词**：对输入文本进行与训练时相同的规范化和预分词处理 。
2. **字符拆分**：将预分词后的每个词拆分为单个字符（或字节，如果是字节级BPE）。
3. **应用合并规则**：按照训练时学习到的合并规则的顺序，依次将这些规则应用于字符序列。如果序列中存在某个规则的左侧部分（即待合并的词元对），则将其替换为规则的右侧部分（即合并后的新词元）。

分词示例 ：

假设我们学习到了以下三条合并规则（按学习顺序）：

1. `("u", "g") -> "ug"`
2. `("u", "n") -> "un"`
3. `("h", "ug") -> "hug"` 并且基础词汇表包含 "b", "h", "s", "u", "g" 等，但不包含 "m", "t"。

- 分词 "bug"：
  1. 初始拆分：`["b", "u", "g"]`
  2. 应用规则 `("u", "g") -> "ug"`：`["b", "ug"]`。最终分词结果。
- 分词 "mug"：
  1. 初始拆分：`["m", "u", "g"]`
  2. "m" 不在基础词汇表中（假设非字节级BPE且训练语料未见"m"），变为 `"[UNK]"`。序列变为 `["[UNK]", "u", "g"]`。
  3. 应用规则 `("u", "g") -> "ug"`：`["[UNK]", "ug"]`。最终分词结果。
- 分词 "thug"：
  1. 初始拆分：`["t", "h", "u", "g"]`
  2. "t" 不在基础词汇表中，变为 `"[UNK]"`。序列变为 `["[UNK]", "h", "u", "g"]`。
  3. 应用规则`("u", "g") -> "ug"`：`["[UNK]", "h", "ug"]`。
  4. 应用规则 `("h", "ug") -> "hug"`：`["[UNK]", "hug"]`。最终分词结果。

BPE通过这种方式，能够有效地将词语分解为已知子词的序列，即使是未登录词，只要其构成字符在基础词汇表中，或者采用字节级BPE，就能被分解和表示，而不是简单地标为`[UNK]`。

### B. WordPiece

WordPiece是另一种流行的子词分词算法，由Google为预训练BERT模型而开发，并被后续的DistilBERT、MobileBERT等模型沿用 。它在训练方法上与BPE有相似之处，都是迭代地构建词汇表，但在选择合并对的标准以及实际分词过程上有所不同 。

#### 核心原理

WordPiece的目标同样是将词切分成子词单元。与BPE主要依赖频率不同，WordPiece在选择合并哪个子词对时，会考虑一个“分数”，这个分数旨在最大化训练数据的似然 。这意味着它倾向于合并那些能够更好地解释语料中词语构成的子词对。

#### 训练步骤详解与示例

WordPiece的训练算法（尽管Google未完全开源其原始实现）大致遵循以下步骤：

1. **初始词汇表与词语预处理**：

   - 训练始于一个小的初始词汇表，包含所有单个字符以及模型可能需要的特殊词元（如BERT中的`, `[UNK]`,`, `,`）。
   - 一个显著的特点是，在WordPiece中，词语内部的子词通常会带有一个特殊前缀，如`##`（BERT使用的约定）。例如，词 "hugging" 在初始拆分后可能表示为 `["hug", "##ging"]`，或者更细致地拆分为 `["h", "##u", "##g", "##g", "##i", "##n", "##g"]`。这个`##`前缀非常关键，它标示了一个子词并非词的开头部分，而是词的中间或末尾部分。这使得模型能够区分例如作为独立词的 "form" 和作为 "perform"一部分的 "##form"，赋予它们不同的语义表示，这对于理解词的结构和意义至关重要，特别适用于像BERT这样需要进行掩码语言模型等任务的模型。

2. **学习合并规则（基于分数）**：

   - 与BPE不同，WordPiece不是简单地合并频率最高的相邻对。它为每一对潜在的合并计算一个分数。一种常见的计分公式是： 

     ​		score=(freq_of_pair)/(freq_of_first_element×freq_of_second_element)

   - 这个分数实际上衡量了这对子词一起出现的频率相对于它们各自独立出现频率的乘积的比例。高分意味着这对子词的“结合度”很高，它们一起出现的概率远大于基于它们独立出现概率所预期的。这种评分机制倾向于合并那些本身不常独立出现，但组合起来却很常见的子词对，从而可能产生更具语义内聚性的子词。

   - 在每一步迭代中，算法选择分数最高的子词对进行合并。合并时，如果内部的子词带有`##`前缀，这个前缀在合并后的新子词中可能会被处理掉（例如，`("play", "##ing")` 合并为 `"playing"`）。

   - 这个过程不断重复，直到词汇表达到预设的大小 。

训练示例 ：

使用与BPE相同的语料：("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)。

假设初始词汇表包含 ["b", "g", "h", "n", "p", "s", "u"] 及特殊前缀##。

初始词语拆分（假设词首无##，词内有##）：

("h ##u ##g", 10), ("p ##u ##g", 5), ("p ##u ##n", 12), ("b ##u ##n", 4), ("h ##u ##g ##s", 5)

- **计算所有可能对的分数**：例如，对 `("##u", "##g")`，假设 `freq(##u ##g) = 20` (来自hug, pug, hugs)，`freq(##u) = 36` (来自所有含u的词的内部u)，`freq(##g) = 20` (来自hug, pug, hugs的内部g)。则 `score(##u, ##g) = 20 / (36 * 20) = 1/36`。 对 `("##g", "##s")`，假设 `freq(##g ##s) = 5` (来自hugs)，`freq(##g) = 20`，`freq(##s) = 5`。则 `score(##g, ##s) = 5 / (20 * 5) = 1/20`。

- 第一次合并：假设 

  ```
  ("##g", "##s")
  ```

   的分数最高 (1/20 > 1/36)。

  - 学习规则：`("##g", "##s") -> "##gs"`。
  - 词汇表增加 "##gs"。
  - 语料更新（概念上）：`("h ##u ##g", 10), ("p ##u ##g", 5), ("p ##u ##n", 12), ("b ##u ##n", 4), ("h ##u ##gs", 5)`。 后续合并会继续基于更新后的频率和新产生的子词对计算分数并选择最优的进行。

#### 分词流程剖析与示例

WordPiece在对新文本进行分词时，与BPE有显著区别：它不依赖于保存下来的合并规则，而是直接使用训练完成的最终词汇表 。

1. **贪婪最长匹配**：对于一个给定的词（通常是经过预分词得到的），WordPiece从词的开头开始，查找能够与词的起始部分匹配的、且存在于最终词汇表中的最长子词 。
2. **迭代处理剩余部分**：找到这个最长子词后，将其作为第一个词元。然后，对于词中剩余的部分，在其前面加上`##`前缀（表示它是词的非起始部分），并对这个带前缀的剩余部分重复步骤1的贪婪最长匹配过程。
3. **处理未登录词**：如果在任何时候，对于当前待处理的（带`##`前缀的）子串，无法在词汇表中找到任何匹配的子词，那么原始的整个词（在预分词层面）可能会被标记为 `[UNK]` 。这一点与BPE不同，BPE通常能将词分解到字符级别，除非字符本身是未知的（在非字节级BPE中）。

分词示例 ：

假设最终词汇表包含: ["hug", "##s", "bug", "##gs", "b", "##u", "[UNK]"] (简化示例)

- 分词 "hugs"：
  1. 从 "hugs" 开头最长匹配到 "hug"（在词汇表中）。
  2. 剩余 "s"。加上前缀变为 "##s"。
  3. "##s" 在词汇表中。
  4. 最终分词：`["hug", "##s"]`。
- 分词 "bugs"：
  1. 从 "bugs" 开头最长匹配到 "bug"（在词汇表中）。
  2. 剩余 "s"。加上前缀变为 "##s"。
  3. "##s" 在词汇表中。
  4. 最终分词：`["bug", "##s"]`。（注意：如果词汇表是像之前训练示例那样产生的，可能是 `["b", "##u", "##gs"]`，取决于训练结果）。
- 分词 "mug"：
  1. 假设从 "mug" 开头，"m", "mu", "mug" 均不在词汇表中。
  2. 整个词 "mug" 被标记为 `[UNK]`。
  3. 最终分词：`["[UNK]"]`。

#### 与BPE的关键区别

- **合并标准**：BPE基于相邻对的原始频率进行合并 。WordPiece则基于一个旨在最大化数据似然的分数来选择合并对 。
- **分词机制**：BPE在分词时应用学习到的合并规则序列 。WordPiece则是在最终词汇表上进行贪婪的最长子词匹配 。
- **对`[UNK]`的处理**：WordPiece如果无法将一个词完全分解为词汇表中的子词（即使是单个字符加上`##`前缀），倾向于将整个词标为`[UNK]`。BPE（尤其是字节级BPE）通常能更细粒度地分解，减少整个词被标为`[UNK]`的情况。

### C. Unigram

Unigram算法是另一种重要的子词分词方法，常与SentencePiece框架结合使用，并被AlBERT、T5、mBART、Big Bird和XLNet等模型采用 。与BPE和WordPiece自底向上（从字符到子词）构建词汇表的方式相反，Unigram采用的是一种自顶向下（从一个大的候选词汇表开始，逐步裁剪）的策略 。

#### 核心原理

Unigram模型的核心思想是，它首先构建一个相对较大的初始候选子词词汇表。然后，它基于一个Unigram语言模型（即假设每个子词的出现是独立的），迭代地移除那些对整个语料库的对数似然损失（log-likelihood loss）影响最小的子词，直到词汇表大小达到预设的目标 。最终的目标是得到一个既能有效表示语料，又具有期望大小的优化词汇表。

#### 训练步骤详解 (含损失函数) 与示例

Unigram的训练过程可以概括为以下几个步骤：

1. **构建初始词汇表**：
   - 训练开始于一个包含大量候选子词的初始词汇表。这个词汇表可以由多种方式生成，例如：包含语料中所有可能的子串，或者使用BPE算法先生成一个非常大的词汇表作为起点 。
   - 为初始词汇表中的每个子词 xi 计算其在训练语料中的概率 P(xi)。这通常是该子词的频率除以所有候选子词的总频率。
2. **定义损失函数**：
   - Unigram训练的目标是最大化给定词汇表 V 时训练语料 C 的对数似然。等价地，是最小化负对数似然损失 L： L=−s∈C∑log(P(s∣V)) 其中 P(s∣V) 是词 s 在当前词汇表 V 和Unigram模型下的概率。对于一个词 s，其概率 P(s∣V) 是通过将其所有可能的子词分割方式中概率最高的那种分割的概率来确定的。如果一个词 s 被分割为子词序列 (x1,x2,…,xk)，则 P(s∣V)=∏i=1kP(xi) （基于Unigram的独立性假设）。
3. **迭代裁剪词汇表**：
   - 在每一步迭代中，对于当前词汇表中的每一个非基础字符的子词 xj，算法会计算如果将 xj 从词汇表中移除，整个语料库的损失 L 会增加多少。这个损失增加量可以看作是子词 xj 的“重要性”或“贡献度”的度量。
   - 选择那些移除后导致损失增加最小的一定比例（例如，10%或20%）的子词，将它们从词汇表中永久删除 。
   - 一个关键的约束是：基础字符（单个字母、符号等）永远不会被移除，以确保任何词语最终都能被分解为基础字符序列，从而避免无法分词的情况 。
   - 重复这个计算损失增加、排序、移除的过程，直到词汇表大小缩减到预设的目标值。

训练示例 ：

沿用之前的语料：("hug", 10), ("pug", 5), ("pun", 12), ("bun", 4), ("hugs", 5)。

假设初始词汇表包含所有子串：["h", "u", "g", "hu", "ug", "p", "pu", "n", "un", "b", "bu", "s", "hug", "gs", "ugs"]。

计算每个子词的频率和概率（总频率为210）：

P("h")=15/210, P("u")=36/210, P("g")=20/210, P("hu")=15/210, P("ug")=20/210,... , P("hug")=15/210。

假设当前词汇表和概率如上。要计算移除某个词（比如 "hug"）的影响：

1. 计算包含 "hug" 时的总损失 Lcurrent。这需要对语料中每个词（如 "hug", "hugs"）使用Viterbi算法找到最优分割并计算其概率，然后加总负对数似然。
2. 临时移除 "hug"，形成新词汇表 V′。
3. 使用 V′ 重新计算语料中所有词的最优分割和概率，得到新的总损失 Lnew。
4. 移除 "hug" 导致的损失增加为 Lnew−Lcurrent。 对所有候选移除的子词都执行此操作，然后选择损失增加最小的那些进行移除。

Unigram的这种基于全局损失优化的裁剪方式，与BPE和WordPiece的贪婪合并形成对比。它试图找到一个在统计上更稳健、更能代表整个语料的子词集合。

#### 分词流程剖析 (含Viterbi算法) 与示例

当使用训练好的Unigram模型对新词进行分词时，流程如下：

1. **寻找所有可能的分割**：对于一个给定的输入词，Unigram模型会考虑所有可能的将其分割成词汇表中存在的子词序列的方式 。
2. **计算每种分割的概率**：由于Unigram假设每个子词的出现是相互独立的，所以一个特定分割方式 (x1,x2,…,xk) 的总概率是其包含的各个子词概率的乘积：P(segmentation)=P(x1)×P(x2)×…×P(xk) 3。
3. **使用Viterbi算法找到最优分割**：为了从众多可能的分割中高效地找出概率最高的那一个，Unigram采用了Viterbi算法 。Viterbi算法是一种动态规划算法，它能够构建一个表示所有可能分割路径的图（格 trellis），并在图中找到最优路径（即概率最高的子词序列）。这确保了对于任何给定的词，Unigram都能找到基于其学习到的子词概率的最优表示，这对于处理词义和结构上的模糊性非常有价值。

分词示例 ：

假设词汇表和概率如训练示例所示。我们要分词 "pug"。

可能的分割有：

- `["p", "u", "g"]`：概率 P("p")×P("u")×P("g")=(17/210)×(36/210)×(20/210)≈0.000389 (假设P("p")=17/210)
- `["p", "ug"]`：概率 P("p")×P("ug")=(17/210)×(20/210)≈0.00771
- `["pu", "g"]`：概率 P("pu")×P("g")=(17/210)×(20/210)≈0.00771 Viterbi算法会比较这些概率，选择最高的。在这个例子中，`["p", "ug"]` 和 `["pu", "g"]` 概率相同且更高，具体选择可能取决于实现（例如，选择第一个找到的）。

再例如分词 "unhug" ：

Viterbi算法会构建一个路径图，找到类似 ["un", "hug"] 这样的分割，其概率 P("un")×P("hug") 是所有可能分割中最高的。

**OOV处理**：如果在分词过程中，`encode_word`函数（通常是Viterbi算法的实现）无法为某个词找到任何有效的分割（即该词无法由词汇表中的子词组成），则该词会被标记为未知词元，例如 `["<unk>"]` 。

Unigram的概率性方法和通过Viterbi算法进行的最优分割选择，使其成为一种在原则上非常优雅的分词策略。

## IV. SentencePiece：现代分词框架

SentencePiece并非一个全新的底层分词算法，而是一个强大的、语言无关的文本分词框架和工具集。它封装并扩展了如BPE和Unigram等子词算法，提供了一套端到端的解决方案，尤其在处理多语言文本和无明显空格分隔的语言方面表现出色 。

### A. 设计理念：原始文本的直接处理

SentencePiece的核心设计理念之一是直接处理原始文本流（raw text stream）。它将输入文本视为一个连续的Unicode字符序列，而不需要预先进行依赖特定语言规则的预分词（pre-tokenization）步骤，例如按空格或标点符号切分 。

这种“无需预分词”的特性是SentencePiece的一大突破，尤其对于那些不使用空格作为词语分隔符的语言（如中文、日文、泰文等）以及词形变化复杂的语言来说，极大地简化了分词流程。传统的NLP流水线中，预分词往往是一个复杂且容易出错的环节，需要针对不同语言定制不同的规则集或引入专门的预分词工具。SentencePiece通过将文本视为纯粹的字符序列，并让模型从数据中直接学习子词边界，避免了这些麻烦。这使得分词过程更加通用，减少了对特定语言专业知识的依赖，是构建真正意义上的多语言NLP系统的重要一步。它将分词的边界学习任务交给了其内部采用的BPE或Unigram算法，使其能够适应各种语言结构。

SentencePiece旨在成为一个端到端的系统，能够直接从原始句子集合中训练分词模型和逆分词模型（detokenizer）。

### B. 关键特性：空格处理、可逆性与语言无关性

SentencePiece的强大功能体现在其几个关键特性上：

- **空格处理 (Whitespace Handling)**：SentencePiece对空格的处理方式独树一帜。它不将空格视为特殊的分隔符并丢弃，而是将其视为词汇表中的一个普通符号来学习。具体做法是，它通常会将空格用一个特殊的元符号（meta symbol）来表示，例如下划线 `` (U+2581) 。这样，原始文本中的空格信息就被编码进了词元序列中。例如，"Hello World" 可能会被编码为 `[  Hello, World]` 这样的词元序列。
- **可逆性 (Reversibility)**：由于空格信息被显式保留，SentencePiece分词的结果是高度可逆的。这意味着从分词后的词元序列可以精确地、无歧义地恢复出原始的（经过规范化处理后的）文本字符串 。逆分词过程通常很简单，只需将词元拼接起来，然后将特殊的空格元符号替换回普通的空格字符即可，例如 `detokenized = ''.join(pieces).replace(' ', ' ')` 。这种可逆性不仅仅是操作上的便利，更重要的是它保证了信息在分词和逆分词过程中的完整性。这对于机器翻译评测（需要精确的原文恢复）和生成模型（需要产生自然且格式正确的文本）等任务至关重要。许多其他分词器可能会在规范化过程中（如将多个空格合并为一个，或去除首尾空格）丢失这些信息，导致无法完美重构原文。
- **语言无关性 (Language Agnosticism)**：SentencePiece的设计不依赖于任何特定语言的语法或词法规则 。它将所有文本都视为Unicode字符流进行处理，使其能够平等地对待各种语言，包括那些没有明确词边界、使用复杂字符集或具有丰富形态变化的语言。

### C. 与BPE/Unigram的集成方式

如前所述，SentencePiece本身并不是一种全新的、独立的子词切分算法。更准确地说，它是一个实现了现有子词算法（主要是BPE和Unigram）并提供了统一接口和额外功能的框架或工具集 。用户在训练SentencePiece模型时，可以选择使用BPE模式或Unigram模式。

SentencePiece对这些基础算法进行了一些重要的扩展和封装：

1. **直接从原始句子训练**：无需预分词。
2. **统一的词汇表大小控制**：用户可以直接指定最终的词汇表大小，而不必关心算法内部的参数（如BPE的合并次数）。
3. **空格的统一处理**：如上所述，将空格符号化。
4. **内置规范化**：例如，默认使用Unicode NFKC进行规范化。
5. **子词正则化（Subword Regularization）**：SentencePiece还支持在训练和推理时进行子词采样（如BPE-dropout或Unigram的nbest_segmentation），即对于同一个词，可以产生多种可能的子词分割，从而增强模型的鲁棒性 。

通过将核心的子词生成逻辑（BPE的合并或Unigram的裁剪）与文本预处理流程（处理原始Unicode、空格、可逆性等）解耦，SentencePiece提供了一个灵活且一致的平台。研究者和开发者可以在这个统一的框架内利用不同子词算法的优势，而不必为每种算法重新实现复杂的文本处理逻辑。它扮演了一个连接底层算法和上层应用需求的中间层角色。

### D. 多语言文本处理的优势与示例

由于其语言无关性、无需预分词以及对原始Unicode流的直接处理能力，SentencePiece在多语言大型语言模型中得到了广泛应用，例如T5、mBERT、XLM-R等模型都采用了SentencePiece作为其分词器 。

示例：

Hugging Face的文档和GitHub示例展示了SentencePiece如何处理文本。例如，使用SentencePiece Python库对"New York"进行编码，并启用采样时，可以看到它如何处理空格（通常表示为）并可能产生不同的分词结果 ：

Python

```
import sentencepiece as spm
# 假设已经训练好了一个SentencePiece模型 spm.model
s = spm.SentencePieceProcessor(model_file='spm.model')
text = "New York"
# 启用采样进行编码，可以看到空格被处理为 ' '
# 并且由于采样，每次结果可能不同（如果模型支持多种分割）
tokens = s.encode(text, out_type=str, enable_sampling=True, alpha=0.1)
# 可能的输出: [' ', 'New', ' York'] 或 [' ', 'N', 'e', 'w', ' York'] 等
```

尽管SentencePiece为多语言NLP带来了巨大进步，但它并非万能药。在处理形态特别丰富或文字系统极其多样的语言（如许多印度语言）时，仍然面临一些挑战 ：

- **词汇不平衡**：在多语言共享词汇表中，高资源语言的词元可能占据主导地位，导致低资源语言的词元覆盖不足。
- **过度切分**：对于黏着语或词形变化复杂的词，SentencePiece（及其底层的BPE/Unigram）仍可能将其切分得过细，潜在地丢失部分语义信息。
- **代码混用和音译处理**：在社交媒体等非正式文本中常见的代码混用（多种语言混合）和罗马字母音译现象，对SentencePiece来说仍是难点。

这表明，虽然SentencePiece是一个强大的通用框架，但在特定语言或特定挑战面前，可能还需要结合更专门的策略或算法上的进一步创新。

## V. 各分词器对比分析与选择指南

理解了各种主流分词算法的内部工作原理后，对它们进行横向比较，并明确其适用场景及处理特殊情况的能力，对于在实践中做出正确选择至关重要。

### A. 性能特点与适用场景对比

下表总结了BPE、WordPiece、Unigram（通常通过SentencePiece实现）以及SentencePiece框架本身的主要特点和适用性：

| **特性**         | **BPE (典型实现)**                              | **WordPiece**                                  | **Unigram (通过SentencePiece)**         | **SentencePiece (框架)**                        |
| ---------------- | ----------------------------------------------- | ---------------------------------------------- | --------------------------------------- | ----------------------------------------------- |
| **核心算法**     | 字节对迭代合并                                  | 基于似然/分数的子词对合并                      | 基于概率模型的子词裁剪                  | 可配置为BPE或Unigram                            |
| **词汇构建方式** | 自底向上，贪婪合并最高频对                      | 自底向上，贪婪合并“最优”分数对                 | 自顶向下，从大词汇表裁剪损失最小的子词  | 依赖所选核心算法 (BPE/Unigram)                  |
| **预分词需求**   | 通常需要 (按空格/标点)                          | 通常需要 (按空格/标点)                         | SentencePiece实现中通常不需要           | 通常不需要 (直接处理原始Unicode流)              |
| **空格处理**     | 通常预分词时处理，可能丢失或用特殊标记表示      | 类似BPE，`##`前缀标记词内部子词                | SentencePiece中空格被视为普通字符 ``    | 空格被视为普通字符 ``，确保可逆性               |
| **可逆性**       | 取决于实现，可能非完全可逆                      | 取决于实现和`##`处理，可能非完全可逆           | SentencePiece中高度可逆                 | 高度可逆                                        |
| **OOV处理**      | 分解为已知子词/字符；字节级BPE可完全消除字符OOV | 分解为已知子词，若无法分解则整个词标为`[UNK]`  | 分解为已知子词，若无法分解则标为`[UNK]` | 依赖核心算法，但通常能分解为已知子词            |
| **多语言支持**   | 基础BPE对多语言支持一般；字节级BPE更佳          | 被认为对多语言有效，但仍需预分词               | SentencePiece使其非常适合多语言         | 设计上语言无关，非常适合多语言                  |
| **主要优点**     | 实现简单，高效处理稀有词，减小词汇表            | 平衡词级和字符级表示，对多语言有效             | 概率建模，词汇选择更优，鲁棒性好        | 语言无关，无需预分词，可逆，灵活支持BPE/Unigram |
| **主要缺点**     | 可能产生无明确语义的子词 9                      | 需大量训练数据学习有意义子词，计算可能较复杂 9 | 实现和训练相对复杂 7                    | 产生的某些token可能不易于人类直接阅读           |
| **典型应用模型** | GPT系列, RoBERTa, BART                          | BERT, DistilBERT                               | ALBERT, T5 (结合SentencePiece)          | T5, mBERT, XLM-R (可使用BPE或Unigram模式)       |



从对比中可以看出，并不存在一个“万能”或“最佳”的分词器。选择哪种分词器是一个需要根据具体应用场景、数据特性（语言种类、语料规模、形态复杂度等）、模型架构以及计算资源综合权衡的工程决策 。例如，BPE因其简单高效而适用于通用场景；WordPiece因其对词内部结构的关注（通过`##`前缀和似然合并）而适用于需要更精细形态学理解的编码器模型；Unigram和SentencePiece则因其强大的多语言处理能力和对原始文本的直接操作而在跨语言任务和处理无空格分隔语言时显示出优势 。

### B. 如何处理特殊情况

#### OOV词汇 (Out-of-Vocabulary Words)

子词分词算法的核心优势之一就是它们处理OOV词汇的能力。它们不是将未知词整个标记为`[UNK]`，而是尝试将其分解为已知的、更小的子词单元 1。

- **BPE**：通过迭代合并，可以将稀有词或新词分解为其组成部分。特别是**字节级BPE**，由于其基础词汇表是所有256个字节，理论上可以表示任何文本序列，从而完全消除字符级别的OOV问题，任何未登录词最终都会被分解为字节序列 。
- **WordPiece**：也会尝试将词分解。但如果一个词（或其带`##`前缀的剩余部分）无法在词汇表中找到任何匹配的子词，WordPiece倾向于将原始的整个词标记为`[UNK]` 。
- **Unigram**：通过Viterbi算法寻找最优分割，如果找不到任何有效的分割方式，则返回`["<unk>"]` 。
- **SentencePiece**：由于它通常采用BPE或Unigram作为核心，其OOV处理方式也继承了这些算法的特点。

#### 多语言文本 (Multilingual Text)

处理多语言文本对分词器提出了更高的要求。

- **SentencePiece** 在这方面表现尤为突出，因为它语言无关，直接处理Unicode字符流，无需预分词，这使其能够公平地对待不同语言的文本，包括那些没有明显空格分隔的语言（如中文、日文、泰文）。
- **WordPiece** 也被认为对多语言有效，因为它能够学习到跨语言共享的子词结构，但它通常仍依赖于预分词步骤，这可能对某些语言不够友好 。
- **BPE** 的基础版本对多语言的支持相对一般，但字节级BPE由于其对任意字符的编码能力，为多语言处理提供了一个更通用的基础。

尽管如此，即使是先进如SentencePiece的分词器，在处理大规模多语言语料时，仍可能面临**词汇不平衡**（高资源语言的词元过多，挤占低资源语言的表示空间）和对某些形态复杂语言**过度切分**等挑战 。这表明在特殊情况的处理上，往往存在通用性与特异性之间的权衡。例如，字节级BPE提供了极致的通用性（能处理任何字符组合），但可能丢失了语言学上的特异性和结构信息。而SentencePiece通过避免预分词实现了通用性，但其底层的BPE或Unigram算法在面对特定语言的复杂形态时，可能不如为该语言专门设计的、更具语言学知识的分词器那样精细（尽管后者会丧失通用性）。

## VI. 总结与展望

### A. 分词技术的核心价值回顾

分词作为自然语言处理流程中的初始环节，其核心价值在于将非结构化的自然语言文本转换为机器能够理解和处理的结构化、数字化的格式 1。它是后续所有高级NLP任务（如文本分类、机器翻译、问答系统、文本生成等）得以实现的基础。

一个优秀的分词策略对大型语言模型的性能起着决定性的作用。它直接影响模型：

- **对语义的理解能力**：合理的子词切分有助于保留词语的形态和语义信息。
- **处理未登录词的能力**：有效的子词机制可以大大减少因OOV词导致的信息损失。
- **计算效率**：合适的词汇表大小和词元序列长度能够优化模型的训练和推理速度。
- **对不同语言的适应性**：尤其在多语言场景下，分词器的设计直接关系到模型能否公平且有效地处理各种语言现象。

从早期的基于词和字符的方法，到如今主流的BPE、WordPiece、Unigram等子词算法，再到SentencePiece这样的集成框架，分词技术一直在不断演进，以期更好地平衡词汇覆盖率、语义保真度、计算效率和对语言多样性的适应能力。然而，正如对多语言和形态丰富语言处理所面临的持续挑战所揭示的那样，分词远非一个已完美解决的问题，它仍然是一个活跃的研究领域 4。

### B. 未来发展趋势简介

展望未来，分词技术可能会朝着以下几个方向发展：

1. **更具上下文感知的分词 (More Context-Aware Tokenization)**：当前主流的子词分词器在训练完成后，其词汇表和分词逻辑基本是固定的。未来的分词器可能会更加动态，能够根据词元所处的具体上下文来调整其切分方式，从而更准确地捕捉词义的细微差别。
2. **针对特定语言或领域优化的自适应分词技术 (Adaptive Tokenization for Specific Languages/Domains)**：虽然通用分词器在多语言场景下表现良好，但针对特定语言（尤其是低资源语言或形态复杂语言）或特定专业领域（如生物医学、法律），可能会出现更精细化的、自适应的或可学习的分词策略，以更好地捕捉该语言或领域的独特词汇和结构特征。
3. **端到端学习分词与模型任务的更紧密结合 (Tighter Integration of End-to-End Learned Tokenization with Model Tasks)**：分词过程可能会更深度地融入到整个大型语言模型的端到端学习中，使得分词策略能够根据最终任务的性能反馈进行优化，而不是作为一个独立的预处理步骤。
4. **对低资源语言和代码混用场景的更好支持 (Improved Support for Low-Resource Languages and Code-Mixing)**：针对目前分词技术在低资源语言覆盖不足以及处理代码混用文本（常见于社交媒体）方面的挑战，未来会有更多研究致力于开发更鲁棒和公平的分词方法 4。
5. **探索“无词元”或隐式分词模型 (Exploration of "Token-Free" or Implicit Tokenization Models)**：一些研究开始探索完全不依赖预定义词元词汇表的模型，例如直接在原始字节流或字符流上操作的模型。这类“Token-Free”模型将分词的负担完全转移给神经网络自身去学习隐式的切分和表示。虽然目前这类模型在计算上可能更为昂贵，但它们提供了最大的灵活性，并可能克服当前基于固定词汇表的子词分词器的一些固有局限性。

总之，当前以SentencePiece为代表的子词分词技术是一个非常有效的折衷方案，但追求更高性能、更强通用性和更好语言理解的努力将持续推动分词技术的创新与发展。



## 参考资料

https://xv44586.github.io/2022/09/08/tokenizers/index.html

https://huggingface.co/learn/llm-course/chapter2/4

https://huggingface.co/learn/llm-course/chapter6/5?fw=pt

https://huggingface.co/learn/llm-course/chapter6/6?fw=pt

https://huggingface.co/learn/llm-course/chapter6/7?fw=pt