import sentencepiece as spm

with open("toy.txt", "w", encoding="utf-8") as f:
  f.write("SentencePiece is an unsupervised text tokenizer and detokenizer mainly for Neural Network-based text generation systems where the vocabulary size is predetermined prior to the neural model training. SentencePiece implements subword units (e.g., byte-pair-encoding (BPE) [Sennrich et al.]) and unigram language model [Kudo.]) with the extension of direct training from raw sentences. SentencePiece allows us to make a purely end-to-end system that does not depend on language-specific pre/postprocessing.")

# train a sentencepiece model on it
# the settings here are (best effort) those used for training Llama 2
import os

options = dict(
  # input spec
  input="toy.txt",
  input_format="text",
  # output spec
  model_prefix="models/sentencepiece_tok400", # output filename prefix
  # algorithm spec
  # BPE algorithm
  model_type="bpe",  # å½“ç„¶è¿˜æœ‰unigramè¿™ç§ç®—æ³•ï¼Œè¿™é‡Œæˆ‘ä»¬é€‰æ‹©BPE
  vocab_size=400,
  # normalization
  normalization_rule_name="identity", # ew, turn off normalizationï¼Œå°±æ˜¯ä¸å¯¹æ–‡æœ¬è¿›è¡Œä»»ä½•å¤„ç†ä¿æŒåŸæ ·
  remove_extra_whitespaces=False,
  input_sentence_size=200000000, # max number of training sentencesï¼Œæœ€å¤§è®­ç»ƒçš„å¥å­æ•°
  max_sentence_length=4192, # max number of bytes per sentenceï¼Œæ¯ä¸ªå¥å­çš„æœ€å¤§å­—èŠ‚æ•°
  seed_sentencepiece_size=1000000, # number of sentences to use for seed sentencepiece model
  shuffle_input_sentence=True, # æ‰“ä¹±è¾“å…¥å¥å­é¡ºåº
  # rare word treatment
  character_coverage=0.99995,   # 99.995% of characters in the training data will be covered by the modelï¼Œæ¯”å¦‚åœ¨1Mä¸ªå¥å­ä¸­ï¼Œ99.995%çš„å­—ç¬¦ä¼šè¢«åŒ…å«åœ¨æ¨¡å‹ä¸­ï¼Œä»…å‡ºç°ä¸€æ¬¡æˆ–è€…å°‘æ•°çš„å­—ç¬¦ä¼šè¢«å¿½ç•¥
  # byte fallback
  byte_fallback=True, # è‹¥ä¸ºtrueï¼Œåˆ™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè‹¥æŸä¸ªå­—ç¬¦æœªè¢«åŒ…å«åœ¨æ¨¡å‹ä¸­ï¼Œåˆ™ä¼šå°†å…¶è½¬æ¢ä¸ºå¯¹åº”çš„å­—èŠ‚åºåˆ—ï¼ˆå›é€€åˆ°256ä¸ªåŸºæœ¬å­—èŠ‚ï¼‰ï¼Œè¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰å­—ç¬¦éƒ½èƒ½è¢«å¤„ç†ï¼Œè‹¥ä¸ºfalseï¼Œåˆ™ä¼šå¿½ç•¥æœªåŒ…å«åœ¨æ¨¡å‹ä¸­çš„å­—ç¬¦ï¼Œè¿™å¯èƒ½å¯¼è‡´æŸäº›å­—ç¬¦æ— æ³•è¢«æ­£ç¡®å¤„ç†ï¼Œæ¯”å¦‚ä¸€é•¿ä¸²ä¸­æ–‡åªä¼šå˜ä¸ºä¸€ä¸ªunkæ ‡è®°ï¼Œidä¸º0çš„æ ‡è®°
  # merge rules
  split_digits=True,            # è¿™äº›è§„åˆ™ç±»ä¼¼bpeçš„regex splitè§„åˆ™
  split_by_unicode_script=True,
  split_by_whitespace=True,
  split_by_number=True,
  max_sentencepiece_length=16,
  add_dummy_prefix=True,# åœ¨æ¯ä¸ªå¥å­å‰æ·»åŠ ä¸€ä¸ªè™šæ‹Ÿå‰ç¼€è¿™é‡Œç”¨çš„æ˜¯ç©ºæ ¼ï¼Œé€šå¸¸ç”¨äºå¤„ç†å¥å­çš„å¼€å¤´
  allow_whitespace_only_pieces=True,
  # special tokens
  unk_id=0, # the UNK token MUST exist
  bos_id=1, # the others are optional, set to -1 to turn off
  eos_id=2,
  pad_id=-1,
  # systems
  num_threads=os.cpu_count(), # use ~all system resources
)

spm.SentencePieceTrainer.train(**options)
sp = spm.SentencePieceProcessor()
sp.load('models/sentencepiece_tok400.model')
vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]
print(vocab[:50])  # print first 50 tokens

ids = sp.encode("hello world!!!?  ğŸ˜‰ä½ å¥½,åƒé¥­äº†å—", out_type=int)
print("encode text:", "hello world!!!?  ğŸ˜‰ä½ å¥½,åƒé¥­äº†å—")
print("encoded ids:", ids)
# Let's also decode the ids back to text
print("decoded text:", sp.decode(ids))
print("è¯¦ç»†æ¯ä¸ªtokençš„idå’Œå¯¹åº”çš„piece:")
print([(idx, sp.id_to_piece(idx)) for idx in ids])